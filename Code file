#Importing the necessary libraries
%matplotlib inline
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

#Importing the dataset
data = pd.read_csv('/content/TSLA.csv')
data.head()


#Describing the feataures of the dataset
description = data.describe()

print(description)

#Importing rcParams to visualize dataset features
from pylab import rcParams

#Visualizing Open
rcParams['figure.figsize'] = 14, 8
sns.set(style='whitegrid', palette='muted', font_scale=1)

ax = data.plot(x="Date", y="Open");
ax.set_xlabel('Date')
ax.set_ylabel('Price ($)')
ax.set_title('Tesla Daily Open Price')

#Visualizing High
ax = data.plot(x="Date", y="High");
ax.set_xlabel('Date')
ax.set_ylabel('Price ($)')
ax.set_title('Tesla Daily High Price')

#Visualizing Low
ax = data.plot(x="Date", y="Low");
ax.set_xlabel('Date')
ax.set_ylabel('Price ($)')
ax.set_title('Tesla Daily Low Price')

#Visualizing Close
ax = data.plot(x="Date", y="Close");
ax.set_xlabel('Date')
ax.set_ylabel('Price ($)')
ax.set_title('Tesla Daily Close Price')

#Visualizing Adjusted Close
ax = data.plot(x="Date", y="Adj Close");
ax.set_xlabel('Date')
ax.set_ylabel('Price ($)')
ax.set_title('Tesla Daily Adjusted Close Price')

#Visualizing Volume
ax = data.plot(x="Date", y="Volume");
ax.set_xlabel('Date')
ax.set_ylabel('Price ($)')
ax.set_title('Tesla Daily Volume')

#Open vs Close
ax = data.plot(x="Date", y=["Open", "Close"]);
ax.set_xlabel('Date')
ax.set_ylabel('Price ($)')
ax.set_title('Tesla Daily Open Price vs Close Price')

#High vs Low
ax = data.plot(x="Date", y=["High", "Low"]);
ax.set_xlabel('Date')
ax.set_ylabel('Price ($)')
ax.set_title('Tesla Daily High Price vs Low Price')

## **Data Preparation and Preprocessing**

#Checking for null values
data.isnull().sum()

#Checking for duplicates
data.duplicated().sum()

#Checking datatypes of the features
data.dtypes

#Normalizing data using MinMaxScaler
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
open_price = data.Open.values.reshape(-1, 1)
scaled_open = scaler.fit_transform(open_price)

close_price = data.Close.values.reshape(-1, 1)
scaled_close = scaler.fit_transform(close_price)

high_price = data.High.values.reshape(-1, 1)
scaled_high = scaler.fit_transform(high_price)

low_price = data.Low.values.reshape(-1, 1)
scaled_low = scaler.fit_transform(low_price)

vol = data.Volume.values.reshape(-1, 1)
scaled_vol = scaler.fit_transform(vol)

 ## **Model Building**

### *Method 1: Bidirectional LSTM*

#Importing necessary libraries for LSTM
from tensorflow import keras
from tensorflow.keras.layers import Bidirectional, Dropout, Activation, Dense, LSTM
from tensorflow.keras.models import Sequential

#Initializing sequence length and train and test sets
seq_length = 120
def splitseq(df, seq_length):
  n_seq = len(df) - seq_length + 1
  return np.array([df[i:(i+seq_length)] for i in range(n_seq)])

def train_test_sets(df, seq_length, train_frc):
  sequences = splitseq(df, seq_length)
  n_train = int(sequences.shape[0] * train_frc)
  x_train = sequences[:n_train, :-1, :]
  y_train = sequences[:n_train, -1, :]
  x_test = sequences[n_train:, :-1, :]
  y_test = sequences[n_train:, -1, :]
  return x_train, y_train, x_test, y_test

X_train, y_train, X_test, y_test = train_test_sets(scaled_open, seq_length, train_frc=0.8)

#Building layers of the model
dropout = 0.2
window_size = seq_length - 1

model = keras.Sequential()

model.add(LSTM(window_size, return_sequences=True, input_shape=(window_size, X_train.shape[-1])))

model.add(Dropout(rate=dropout))
model.add(Bidirectional(LSTM(window_size * 2, return_sequences=True)))

model.add(Dropout(rate=dropout))
model.add(Bidirectional(LSTM(window_size, return_sequences=False)))

model.add(Dense(units=1))
model.add(Activation('linear'))

#Model fitting and validation
batch_size = 16
model.compile(loss='mean_squared_error', optimizer='adam')

history = model.fit(X_train, y_train, epochs=30, batch_size=batch_size, shuffle=False, validation_split=0.2)

#Plotting actual vs predicted prices
y_pred = model.predict(X_test)

y_test_org = scaler.inverse_transform(y_test)
y_pred_org = scaler.inverse_transform(y_pred)

plt.plot(y_test_org, label='Actual Price', color='blue')
plt.plot(y_pred_org, label='Predicted Price', color='green')

plt.title('Tesla Open Price Prediction')
plt.xlabel('Date')
plt.ylabel('Price ($)')
plt.legend(loc='best')

plt.show()

#Plotting change in MSE over each epochs
plt.plot(history.history['loss'], label='Training MSE')
plt.plot(history.history['val_loss'], label='Validation MSE')
plt.title('Model Mean Squared Error')
plt.ylabel('MSE')
plt.xlabel('Epoch')
plt.legend(loc='upper right')
plt.show()

#Displaying final MSE
print("Final Training MSE:", history.history['loss'][-1])
print("Final Validation MSE:", history.history['val_loss'][-1])

#Initializing sequence length and train and test sets
seq_length = 120
def splitseq(df, seq_length):
  n_seq = len(df) - seq_length + 1
  return np.array([df[i:(i+seq_length)] for i in range(n_seq)])

def train_test_sets(df, seq_length, train_frc):
  sequences = splitseq(df, seq_length)
  n_train = int(sequences.shape[0] * train_frc)
  x_train = sequences[:n_train, :-1, :]
  y_train = sequences[:n_train, -1, :]
  x_test = sequences[n_train:, :-1, :]
  y_test = sequences[n_train:, -1, :]
  return x_train, y_train, x_test, y_test

X_train, y_train, X_test, y_test = train_test_sets(scaled_close, seq_length, train_frc=0.8)

#Building layers of the model
dropout = 0.2
window_size = seq_length - 1

model = keras.Sequential()

model.add(LSTM(window_size, return_sequences=True, input_shape=(window_size, X_train.shape[-1])))

model.add(Dropout(rate=dropout))
model.add(Bidirectional(LSTM(window_size * 2, return_sequences=True)))

model.add(Dropout(rate=dropout))
model.add(Bidirectional(LSTM(window_size, return_sequences=False)))

model.add(Dense(units=1))
model.add(Activation('linear'))

#Model fitting and validation
batch_size = 16
model.compile(loss='mean_squared_error', optimizer='adam')

history = model.fit(X_train, y_train, epochs=25, batch_size=batch_size, shuffle=False, validation_split=0.2)

#Plotting actual vs predicted prices
y_pred = model.predict(X_test)

y_test_org = scaler.inverse_transform(y_test)
y_pred_org = scaler.inverse_transform(y_pred)

plt.plot(y_test_org, label='Actual Price', color='blue')
plt.plot(y_pred_org, label='Predicted Price', color='green')

plt.title('Tesla Closing Price Prediction')
plt.xlabel('Date')
plt.ylabel('Price ($)')
plt.legend(loc='best')

plt.show()

#Plotting change in MSE over each epochs
plt.plot(history.history['loss'], label='Training MSE')
plt.plot(history.history['val_loss'], label='Validation MSE')
plt.title('Model Mean Squared Error')
plt.ylabel('MSE')
plt.xlabel('Epoch')
plt.legend(loc='upper right')
plt.show()

#Displaying final MSE
print("Final Training MSE:", history.history['loss'][-1])
print("Final Validation MSE:", history.history['val_loss'][-1])

#Initializing sequence length and train and test sets
seq_length = 120
def splitseq(df, seq_length):
  n_seq = len(df) - seq_length + 1
  return np.array([df[i:(i+seq_length)] for i in range(n_seq)])

def train_test_sets(df, seq_length, train_frc):
  sequences = splitseq(df, seq_length)
  n_train = int(sequences.shape[0] * train_frc)
  x_train = sequences[:n_train, :-1, :]
  y_train = sequences[:n_train, -1, :]
  x_test = sequences[n_train:, :-1, :]
  y_test = sequences[n_train:, -1, :]
  return x_train, y_train, x_test, y_test

X_train, y_train, X_test, y_test = train_test_sets(scaled_high, seq_length, train_frc=0.8)

#Building layers of the model
dropout = 0.2
window_size = seq_length - 1

model = keras.Sequential()

model.add(LSTM(window_size, return_sequences=True, input_shape=(window_size, X_train.shape[-1])))

model.add(Dropout(rate=dropout))
model.add(Bidirectional(LSTM(window_size * 2, return_sequences=True)))

model.add(Dropout(rate=dropout))
model.add(Bidirectional(LSTM(window_size, return_sequences=False)))

model.add(Dense(units=1))
model.add(Activation('linear'))

#Model fitting and validation
batch_size = 16
model.compile(loss='mean_squared_error', optimizer='adam')

history = model.fit(X_train, y_train, epochs=20, batch_size=batch_size, shuffle=False, validation_split=0.2)

#Plotting actual vs predicted prices
y_pred = model.predict(X_test)

y_test_org = scaler.inverse_transform(y_test)
y_pred_org = scaler.inverse_transform(y_pred)

plt.plot(y_test_org, label='Actual Price', color='blue')
plt.plot(y_pred_org, label='Predicted Price', color='green')

plt.title('Tesla High Price Prediction')
plt.xlabel('Date')
plt.ylabel('Price ($)')
plt.legend(loc='best')

plt.show()

#Plotting change in MSE over each epochs
plt.plot(history.history['loss'], label='Training MSE')
plt.plot(history.history['val_loss'], label='Validation MSE')
plt.title('Model Mean Squared Error')
plt.ylabel('MSE')
plt.xlabel('Epoch')
plt.legend(loc='upper right')
plt.show()

#Displaying final MSE
print("Final Training MSE:", history.history['loss'][-1])
print("Final Validation MSE:", history.history['val_loss'][-1])

#Initializing sequence length and train and test sets
seq_length = 120
def splitseq(df, seq_length):
  n_seq = len(df) - seq_length + 1
  return np.array([df[i:(i+seq_length)] for i in range(n_seq)])

def train_test_sets(df, seq_length, train_frc):
  sequences = splitseq(df, seq_length)
  n_train = int(sequences.shape[0] * train_frc)
  x_train = sequences[:n_train, :-1, :]
  y_train = sequences[:n_train, -1, :]
  x_test = sequences[n_train:, :-1, :]
  y_test = sequences[n_train:, -1, :]
  return x_train, y_train, x_test, y_test

X_train, y_train, X_test, y_test = train_test_sets(scaled_low, seq_length, train_frc=0.8)

#Building layers of the model
dropout = 0.2
window_size = seq_length - 1

model = keras.Sequential()

model.add(LSTM(window_size, return_sequences=True, input_shape=(window_size, X_train.shape[-1])))

model.add(Dropout(rate=dropout))
model.add(Bidirectional(LSTM(window_size * 2, return_sequences=True)))

model.add(Dropout(rate=dropout))
model.add(Bidirectional(LSTM(window_size, return_sequences=False)))

model.add(Dense(units=1))
model.add(Activation('linear'))

#Model fitting and validation
batch_size = 16
model.compile(loss='mean_squared_error', optimizer='adam')

history = model.fit(X_train, y_train, epochs=20, batch_size=batch_size, shuffle=False, validation_split=0.2)

#Plotting actual vs predicted prices
y_pred = model.predict(X_test)

y_test_org = scaler.inverse_transform(y_test)
y_pred_org = scaler.inverse_transform(y_pred)

plt.plot(y_test_org, label='Actual Price', color='blue')
plt.plot(y_pred_org, label='Predicted Price', color='green')

plt.title('Tesla Low Price Prediction')
plt.xlabel('Date')
plt.ylabel('Price ($)')
plt.legend(loc='best')

plt.show()

#Plotting change in MSE over each epochs
plt.plot(history.history['loss'], label='Training MSE')
plt.plot(history.history['val_loss'], label='Validation MSE')
plt.title('Model Mean Squared Error')
plt.ylabel('MSE')
plt.xlabel('Epoch')
plt.legend(loc='upper right')
plt.show()

#Displaying final MSE
print("Final Training MSE:", history.history['loss'][-1])
print("Final Validation MSE:", history.history['val_loss'][-1])

#Initializing sequence length and train and test sets
seq_length = 120
def splitseq(df, seq_length):
  n_seq = len(df) - seq_length + 1
  return np.array([df[i:(i+seq_length)] for i in range(n_seq)])

def train_test_sets(df, seq_length, train_frc):
  sequences = splitseq(df, seq_length)
  n_train = int(sequences.shape[0] * train_frc)
  x_train = sequences[:n_train, :-1, :]
  y_train = sequences[:n_train, -1, :]
  x_test = sequences[n_train:, :-1, :]
  y_test = sequences[n_train:, -1, :]
  return x_train, y_train, x_test, y_test

X_train, y_train, X_test, y_test = train_test_sets(scaled_vol, seq_length, train_frc=0.8)

#Building layers of the model
dropout = 0.2
window_size = seq_length - 1

model = keras.Sequential()

model.add(LSTM(window_size, return_sequences=True, input_shape=(window_size, X_train.shape[-1])))

model.add(Dropout(rate=dropout))
model.add(Bidirectional(LSTM(window_size * 2, return_sequences=True)))

model.add(Dropout(rate=dropout))
model.add(Bidirectional(LSTM(window_size, return_sequences=False)))

model.add(Dense(units=1))
model.add(Activation('linear'))

#Model fitting and validation
batch_size = 16
model.compile(loss='mean_squared_error', optimizer='adam')

history = model.fit(X_train, y_train, epochs=20, batch_size=batch_size, shuffle=False, validation_split=0.2)

#Plotting actual vs predicted prices
y_pred = model.predict(X_test)

y_test_org = scaler.inverse_transform(y_test)
y_pred_org = scaler.inverse_transform(y_pred)

plt.plot(y_test_org, label='Actual Price', color='blue')
plt.plot(y_pred_org, label='Predicted Price', color='green')

plt.title('Tesla Volume Prediction')
plt.xlabel('Date')
plt.ylabel('Volume')
plt.legend(loc='best')

plt.show()

#Plotting change in MSE over each epochs
plt.plot(history.history['loss'], label='Training MSE')
plt.plot(history.history['val_loss'], label='Validation MSE')
plt.title('Model Mean Squared Error')
plt.ylabel('MSE')
plt.xlabel('Epoch')
plt.legend(loc='upper right')
plt.show()

#Displaying final MSE
print("Final Training MSE:", history.history['loss'][-1])
print("Final Validation MSE:", history.history['val_loss'][-1])

### *Method 2: Support Vector Regression*

# Importing SVR and Mean Squared Error
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error

#Model initialization
model = SVR(kernel='rbf', gamma=0.5, C=10, epsilon=0.05)

#Training and Test split
open_train = scaled_open[:900]
open_test = scaled_open[900:]
close_train = scaled_close[:900]
close_test = scaled_close[900:]
high_train = scaled_high[:900]
high_test = scaled_high[900:]
low_train = scaled_low[:900]
low_test = scaled_low[900:]
volume_train = scaled_vol[:900]
volume_test = scaled_vol[900:]

#Creating X_train, X_test, y_train, y_test
def create_dataset(X, y, time_steps=1):
    Xs, ys = [], []
    for i in range(len(X) - time_steps):
        v = X[i:i + time_steps]
        Xs.append(v)
        ys.append(y[i + time_steps])
    return np.array(Xs), np.array(ys)

TIME_STEPS = 5
X_train, y_train = create_dataset(open_train, open_train, TIME_STEPS)
X_test, y_test = create_dataset(open_test, open_test, TIME_STEPS)
print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape)

#Model fitting
model.fit(X_train.reshape(X_train.shape[0], -1), y_train)

#Predicting values and inverse scaling
train_pred = model.predict(X_train.reshape(X_train.shape[0], -1))
test_pred = model.predict(X_test.reshape(X_test.shape[0], -1))

train_pred_inv = scaler.inverse_transform(train_pred.reshape(-1, 1))
y_train_inv = scaler.inverse_transform(y_train.reshape(-1, 1))
test_pred_inv = scaler.inverse_transform(test_pred.reshape(-1, 1))
y_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))

#Calculating MSE
mse_train = mean_squared_error(y_train_inv, train_pred_inv)
mse_test = mean_squared_error(y_test_inv, test_pred_inv)
print("Mean Squared Error on Training Data:", mse_train)
print("Mean Squared Error on Testing Data:", mse_test)

#Plotting actual vs predicted for training set
plt.figure(figsize=(15, 8))
plt.plot(data.index[TIME_STEPS:TIME_STEPS+len(train_pred_inv)], y_train_inv, label='Original', color='darkgreen')
plt.plot(data.index[TIME_STEPS:TIME_STEPS+len(train_pred_inv)], train_pred_inv, label='Predicted', color='red')
plt.xlabel('Date')
plt.ylabel('Open Price ($)')
plt.title('Tesla Open Price Forecasting with SVR')
plt.legend()
plt.show()

#Plotting actual vs predicted for test set
y_test_inv = y_test_inv.flatten()

x_values = data.index[TIME_STEPS+len(train_pred_inv)+TIME_STEPS-1 : TIME_STEPS+len(train_pred_inv)+TIME_STEPS-1 + len(y_test_inv)]

plt.figure(figsize=(15, 8))
plt.plot(x_values, y_test_inv, label='Original', color='darkgreen')
plt.plot(x_values, test_pred_inv, label='Predicted', color='red')
plt.xlabel('Date')
plt.ylabel('Open Price ($)')
plt.title('Tesla Open Price Forecasting with SVR - Test Dataset')
plt.legend()
plt.show()

#Creating X_train, X_test, y_train, y_test
def create_dataset(X, y, time_steps=1):
    Xs, ys = [], []
    for i in range(len(X) - time_steps):
        v = X[i:i + time_steps]
        Xs.append(v)
        ys.append(y[i + time_steps])
    return np.array(Xs), np.array(ys)

TIME_STEPS = 5
X_train, y_train = create_dataset(close_train, close_train, TIME_STEPS)
X_test, y_test = create_dataset(close_test, close_test, TIME_STEPS)
print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape)

#Model fitting
model.fit(X_train.reshape(X_train.shape[0], -1), y_train)

#Predicting values and inverse scaling
train_pred = model.predict(X_train.reshape(X_train.shape[0], -1))
test_pred = model.predict(X_test.reshape(X_test.shape[0], -1))

train_pred_inv = scaler.inverse_transform(train_pred.reshape(-1, 1))
y_train_inv = scaler.inverse_transform(y_train.reshape(-1, 1))
test_pred_inv = scaler.inverse_transform(test_pred.reshape(-1, 1))
y_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))

#Calculating MSE
mse_train = mean_squared_error(y_train_inv, train_pred_inv)
mse_test = mean_squared_error(y_test_inv, test_pred_inv)
print("Mean Squared Error on Training Data:", mse_train)
print("Mean Squared Error on Testing Data:", mse_test)

#Plotting actual vs predicted for training set
plt.figure(figsize=(15, 8))
plt.plot(data.index[TIME_STEPS:TIME_STEPS+len(train_pred_inv)], y_train_inv, label='Original', color='darkgreen')
plt.plot(data.index[TIME_STEPS:TIME_STEPS+len(train_pred_inv)], train_pred_inv, label='Predicted', color='red')
plt.xlabel('Date')
plt.ylabel('Close Price ($)')
plt.title('Tesla Close Price Forecasting with SVR')
plt.legend()
plt.show()

#Plotting actual vs predicted for test set
y_test_inv = y_test_inv.flatten()

x_values = data.index[TIME_STEPS+len(train_pred_inv)+TIME_STEPS-1 : TIME_STEPS+len(train_pred_inv)+TIME_STEPS-1 + len(y_test_inv)]

plt.figure(figsize=(15, 8))
plt.plot(x_values, y_test_inv, label='Original', color='darkgreen')
plt.plot(x_values, test_pred_inv, label='Predicted', color='red')
plt.xlabel('Date')
plt.ylabel('Close Price ($)')
plt.title('Tesla Close Price Forecasting with SVR - Test Dataset')
plt.legend()
plt.show()

#Creating X_train, X_test, y_train, y_test
def create_dataset(X, y, time_steps=1):
    Xs, ys = [], []
    for i in range(len(X) - time_steps):
        v = X[i:i + time_steps]
        Xs.append(v)
        ys.append(y[i + time_steps])
    return np.array(Xs), np.array(ys)

TIME_STEPS = 5
X_train, y_train = create_dataset(high_train, high_train, TIME_STEPS)
X_test, y_test = create_dataset(high_test, high_test, TIME_STEPS)
print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape)

#Model fitting
model.fit(X_train.reshape(X_train.shape[0], -1), y_train)

#Predicting values and inverse scaling
train_pred = model.predict(X_train.reshape(X_train.shape[0], -1))
test_pred = model.predict(X_test.reshape(X_test.shape[0], -1))

train_pred_inv = scaler.inverse_transform(train_pred.reshape(-1, 1))
y_train_inv = scaler.inverse_transform(y_train.reshape(-1, 1))
test_pred_inv = scaler.inverse_transform(test_pred.reshape(-1, 1))
y_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))

#Calculating MSE
mse_train = mean_squared_error(y_train_inv, train_pred_inv)
mse_test = mean_squared_error(y_test_inv, test_pred_inv)
print("Mean Squared Error on Training Data:", mse_train)
print("Mean Squared Error on Testing Data:", mse_test)

#Plotting actual vs predicted for training set
plt.figure(figsize=(15, 8))
plt.plot(data.index[TIME_STEPS:TIME_STEPS+len(train_pred_inv)], y_train_inv, label='Original', color='darkgreen')
plt.plot(data.index[TIME_STEPS:TIME_STEPS+len(train_pred_inv)], train_pred_inv, label='Predicted', color='red')
plt.xlabel('Date')
plt.ylabel('High Price ($)')
plt.title('Tesla High Price Forecasting with SVR')
plt.legend()
plt.show()

#Plotting actual vs predicted for test set
y_test_inv = y_test_inv.flatten()

x_values = data.index[TIME_STEPS+len(train_pred_inv)+TIME_STEPS-1 : TIME_STEPS+len(train_pred_inv)+TIME_STEPS-1 + len(y_test_inv)]

plt.figure(figsize=(15, 8))
plt.plot(x_values, y_test_inv, label='Original', color='darkgreen')
plt.plot(x_values, test_pred_inv, label='Predicted', color='red')
plt.xlabel('Date')
plt.ylabel('High Price ($)')
plt.title('Tesla High Price Forecasting with SVR - Test Dataset')
plt.legend()
plt.show()

#Creating X_train, X_test, y_train, y_test
def create_dataset(X, y, time_steps=1):
    Xs, ys = [], []
    for i in range(len(X) - time_steps):
        v = X[i:i + time_steps]
        Xs.append(v)
        ys.append(y[i + time_steps])
    return np.array(Xs), np.array(ys)

TIME_STEPS = 5
X_train, y_train = create_dataset(low_train, low_train, TIME_STEPS)
X_test, y_test = create_dataset(low_test, low_test, TIME_STEPS)
print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape)

#Model fitting
model.fit(X_train.reshape(X_train.shape[0], -1), y_train)

#Predicting values and inverse scaling
train_pred = model.predict(X_train.reshape(X_train.shape[0], -1))
test_pred = model.predict(X_test.reshape(X_test.shape[0], -1))

train_pred_inv = scaler.inverse_transform(train_pred.reshape(-1, 1))
y_train_inv = scaler.inverse_transform(y_train.reshape(-1, 1))
test_pred_inv = scaler.inverse_transform(test_pred.reshape(-1, 1))
y_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))

#Calculating MSE
mse_train = mean_squared_error(y_train_inv, train_pred_inv)
mse_test = mean_squared_error(y_test_inv, test_pred_inv)
print("Mean Squared Error on Training Data:", mse_train)
print("Mean Squared Error on Testing Data:", mse_test)

#Plotting actual vs predicted for training set
plt.figure(figsize=(15, 8))
plt.plot(data.index[TIME_STEPS:TIME_STEPS+len(train_pred_inv)], y_train_inv, label='Original', color='darkgreen')
plt.plot(data.index[TIME_STEPS:TIME_STEPS+len(train_pred_inv)], train_pred_inv, label='Predicted', color='red')
plt.xlabel('Date')
plt.ylabel('Low Price ($)')
plt.title('Tesla Low Price Forecasting with SVR')
plt.legend()
plt.show()

#Plotting actual vs predicted for test set
y_test_inv = y_test_inv.flatten()

x_values = data.index[TIME_STEPS+len(train_pred_inv)+TIME_STEPS-1 : TIME_STEPS+len(train_pred_inv)+TIME_STEPS-1 + len(y_test_inv)]

plt.figure(figsize=(15, 8))
plt.plot(x_values, y_test_inv, label='Original', color='darkgreen')
plt.plot(x_values, test_pred_inv, label='Predicted', color='red')
plt.xlabel('Date')
plt.ylabel('Low Price ($)')
plt.title('Tesla Low Price Forecasting with SVR - Test Dataset')
plt.legend()
plt.show()

#Creating X_train, X_test, y_train, y_test
def create_dataset(X, y, time_steps=1):
    Xs, ys = [], []
    for i in range(len(X) - time_steps):
        v = X[i:i + time_steps]
        Xs.append(v)
        ys.append(y[i + time_steps])
    return np.array(Xs), np.array(ys)

TIME_STEPS = 5
X_train, y_train = create_dataset(volume_train, volume_train, TIME_STEPS)
X_test, y_test = create_dataset(volume_test, volume_test, TIME_STEPS)
print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape)

#Model fitting
model.fit(X_train.reshape(X_train.shape[0], -1), y_train)

#Predicting values and inverse scaling
train_pred = model.predict(X_train.reshape(X_train.shape[0], -1))
test_pred = model.predict(X_test.reshape(X_test.shape[0], -1))

train_pred_inv = scaler.inverse_transform(train_pred.reshape(-1, 1))
y_train_inv = scaler.inverse_transform(y_train.reshape(-1, 1))
test_pred_inv = scaler.inverse_transform(test_pred.reshape(-1, 1))
y_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))

#Calculating MSE
mse_train = mean_squared_error(y_train_inv, train_pred_inv)
mse_test = mean_squared_error(y_test_inv, test_pred_inv)
print("Mean Squared Error on Training Data:", mse_train)
print("Mean Squared Error on Testing Data:", mse_test)

#Plotting actual vs predicted for training set
plt.figure(figsize=(15, 8))
plt.plot(data.index[TIME_STEPS:TIME_STEPS+len(train_pred_inv)], y_train_inv, label='Original', color='darkgreen')
plt.plot(data.index[TIME_STEPS:TIME_STEPS+len(train_pred_inv)], train_pred_inv, label='Predicted', color='red')
plt.xlabel('Date')
plt.ylabel('Low Price ($)')
plt.title('Tesla Volume Forecasting with SVR')
plt.legend()
plt.show()

#Plotting actual vs predicted for test set
y_test_inv = y_test_inv.flatten()

x_values = data.index[TIME_STEPS+len(train_pred_inv)+TIME_STEPS-1 : TIME_STEPS+len(train_pred_inv)+TIME_STEPS-1 + len(y_test_inv)]

plt.figure(figsize=(15, 8))
plt.plot(x_values, y_test_inv, label='Original', color='darkgreen')
plt.plot(x_values, test_pred_inv, label='Predicted', color='red')
plt.xlabel('Date')
plt.ylabel('Low Price ($)')
plt.title('Tesla Volume Forecasting with SVR - Test Dataset')
plt.legend()
plt.show()

### *Method 3: Time Series Regression using Extreme Gradient Boosting*

data.head()

!pip install pycaret

data.dtypes

df = data.copy()

# create 5 year moving average
df['MA5'] = df['Open'].rolling(2).mean()
# plot the df and MA
import plotly.express as px
fig = px.line(df, x="Date", y=["Open", "MA5"], template = 'plotly_dark')
fig.show()

# create a sequence of numbers
df['Series'] = np.arange(1,len(df)+1)
# drop unnecessary columns and re-arrange
df.drop(['Close','High','Low','Adj Close','Volume'],axis=1, inplace=True)
df = df[['Series','Date','Open','MA5']]
# check the head of the dfset
df.head()

df['Date'] = pd.to_datetime(df['Date'], dayfirst=True)
df['Date'] = df['Date'].dt.strftime('%d%m%Y').astype(int)

# split data into train-test set manually
train = df[:900]
test = df[900:]
# check shape
train.shape, test.shape

test.head()

# import the regression module
from pycaret.regression import *
# initialize setup
s = setup(data=train, test_data=test, target='Open', fold_strategy='timeseries', numeric_features=['Date', 'Series'], fold=3, transform_target=True, fold_shuffle=False, data_split_shuffle=False, session_id=123)

best = create_model('xgboost')

prediction_holdout = predict_model(best);

# generate predictions on the original dataset
predictions = predict_model(best, data=df)
# add a date column in the dataset
predictions['Date'] = pd.date_range(start='2019-04-01', periods=len(predictions), freq = 'B')
# line plot
fig = px.line(predictions, x='Date', y=["Open","MA5"], template = 'plotly_dark')
# add a vertical rectange for test-set separation
fig.add_vrect(x0="2022-10-25", x1="2024-03-28", fillcolor="grey", opacity=0.25, line_width=0)
fig.show()

final_best = finalize_model(best)

future_dates = pd.date_range(start = '2024-03-28', end = '2028-03-31', freq = 'B')
future_df = pd.DataFrame()
future_df['Date'] = pd.date_range(start = '2024-03-28', end = '2028-03-31', freq = 'B')
future_df['Series'] = np.arange(2000,(2000+len(future_dates)))
future_df.head()

future_df['MA5'] = df['Open'].rolling(2).mean()

future_df['Date'] = future_df['Date'].dt.strftime('%d%m%Y').astype(int)

predictions_future = predict_model(final_best, data=future_df)
predictions_future.head()

concat_df = pd.concat([df,predictions_future], axis=0)
concat_df_i = pd.date_range(start='2019-04-01', periods=len(concat_df), freq='B')
concat_df.set_index(concat_df_i, inplace=True)
fig = px.line(concat_df, x=concat_df.index, y=["Open", "prediction_label"], template = 'plotly_dark')
fig.show()

df = data.copy()

# create 10 year moving average
df['MA5'] = df['Close'].rolling(2).mean()
# plot the data and MA
import plotly.express as px
fig = px.line(df, x="Date", y=["Close", "MA5"], template = 'plotly_dark')
fig.show()

# create a sequence of numbers
df['Series'] = np.arange(1,len(df)+1)
# drop unnecessary columns and re-arrange
df.drop(['Open','High','Low','Adj Close','Volume'],axis=1, inplace=True)
df = df[['Series','Date','Close','MA5']]
# check the head of the dataset
df.head()

df['Date'] = pd.to_datetime(df['Date'], dayfirst=True)
df['Date'] = df['Date'].dt.strftime('%d%m%Y').astype(int)

# split data into train-test set manually
train = df[:900]
test = df[900:]
# check shape
train.shape, test.shape

# initialize setup
s = setup(data=train, test_data=test, target='Close', fold_strategy='timeseries', numeric_features=['Date', 'Series'], fold=3, transform_target=True, fold_shuffle=False, data_split_shuffle=False, session_id=123)

best = create_model('xgboost')

prediction_holdout = predict_model(best);

# generate predictions on the original dataset
predictions = predict_model(best, data=df)
# add a date column in the dataset
predictions['Date'] = pd.date_range(start='2019-04-01', periods=len(predictions), freq = 'B')
# predictions['year_ended'] = pd.date_range(start='2013-03-31', periods=len(predictions), freq='A')
# line plot
fig = px.line(predictions, x='Date', y=["Close","MA5"], template = 'plotly_dark')
# add a vertical rectange for test-set separation
fig.add_vrect(x0="2022-10-25", x1="2024-03-28", fillcolor="grey", opacity=0.25, line_width=0)
fig.show()

final_best = finalize_model(best)

future_dates = pd.date_range(start = '2024-03-28', end = '2028-03-31', freq = 'B')
future_df = pd.DataFrame()
future_df['Date'] = pd.date_range(start = '2024-03-28', end = '2028-03-31', freq = 'B')
future_df['Series'] = np.arange(2000,(2000+len(future_dates)))
future_df.head()

future_df['MA5'] = df['Close'].rolling(2).mean()

future_df['Date'] = future_df['Date'].dt.strftime('%Y%m%d').astype(int)

predictions_future = predict_model(final_best, data=future_df)
predictions_future.head()

concat_df = pd.concat([df,predictions_future], axis=0)
concat_df_i = pd.date_range(start='2019-04-01', periods=len(concat_df), freq='B')
concat_df.set_index(concat_df_i, inplace=True)
fig = px.line(concat_df, x=concat_df.index, y=["Close", "prediction_label"], template = 'plotly_dark')
fig.show()

df = data.copy()

# create 10 year moving average
df['MA5'] = df['High'].rolling(2).mean()
# plot the data and MA
import plotly.express as px
fig = px.line(df, x="Date", y=["High", "MA5"], template = 'plotly_dark')
fig.show()

# create a sequence of numbers
df['Series'] = np.arange(1,len(df)+1)
# drop unnecessary columns and re-arrange
df.drop(['Open','Close','Low','Adj Close','Volume'],axis=1, inplace=True)
df = df[['Series','Date','High','MA5']]
# check the head of the dataset
df.head()

df['Date'] = pd.to_datetime(df['Date'], dayfirst = True)
df['Date'] = df['Date'].dt.strftime('%d%m%Y').astype(int)

# split data into train-test set manually
train = df[:900]
test = df[900:]
# check shape
train.shape, test.shape

s = setup(data=train, test_data=test, target='High', fold_strategy='timeseries', numeric_features=['Date', 'Series'], fold=3, transform_target=True, fold_shuffle=False, data_split_shuffle=False, session_id=123)

best = create_model('xgboost')

prediction_holdout = predict_model(best);

# generate predictions on the original dataset
predictions = predict_model(best, data=df)
# add a date column in the dataset
predictions['Date'] = pd.date_range(start='2019-04-01', periods=len(predictions), freq = 'B')
# predictions['year_ended'] = pd.date_range(start='2013-03-31', periods=len(predictions), freq='A')
# line plot
fig = px.line(predictions, x='Date', y=["High","MA5"], template = 'plotly_dark')
# add a vertical rectange for test-set separation
fig.add_vrect(x0="2022-10-25", x1="2024-03-28", fillcolor="grey", opacity=0.25, line_width=0)
fig.show()

final_best = finalize_model(best)

future_dates = pd.date_range(start = '2024-03-28', end = '2028-03-31', freq = 'B')
future_df = pd.DataFrame()
future_df['Date'] = pd.date_range(start = '2024-03-28', end = '2028-03-31', freq = 'B')
future_df['Series'] = np.arange(2000,(2000+len(future_dates)))
future_df.head()

future_df['MA5'] = df['High'].rolling(2).mean()

future_df['Date'] = future_df['Date'].dt.strftime('%Y%m%d').astype(int)

predictions_future = predict_model(final_best, data=future_df)
predictions_future.head()

concat_df = pd.concat([df,predictions_future], axis=0)
concat_df_i = pd.date_range(start='2019-04-01', periods=len(concat_df), freq='B')
concat_df.set_index(concat_df_i, inplace=True)
fig = px.line(concat_df, x=concat_df.index, y=["High", "prediction_label"], template = 'plotly_dark')
fig.show()

df = data.copy()

# create 10 year moving average
df['MA5'] = df['Low'].rolling(2).mean()
# plot the data and MA
import plotly.express as px
fig = px.line(df, x="Date", y=["Low", "MA5"], template = 'plotly_dark')
fig.show()

# create a sequence of numbers
df['Series'] = np.arange(1,len(df)+1)
# drop unnecessary columns and re-arrange
df.drop(['Open','Close','High','Adj Close','Volume'],axis=1, inplace=True)
df = df[['Series','Date','Low','MA5']]
# check the head of the dataset
df.head()

df['Date'] = pd.to_datetime(df['Date'], dayfirst = True)
df['Date'] = df['Date'].dt.strftime('%Y%m%d').astype(int)

# split data into train-test set manually
train = df[:900]
test = df[900:]
# check shape
train.shape, test.shape

s = setup(data=train, test_data=test, target='Low', fold_strategy='timeseries', numeric_features=['Date', 'Series'], fold=3, transform_target=True, fold_shuffle=False, data_split_shuffle=False, session_id=123)

best = create_model('xgboost')

prediction_holdout = predict_model(best);

# generate predictions on the original dataset
predictions = predict_model(best, data=df)
# add a date column in the dataset
predictions['Date'] = pd.date_range(start='2019-04-01', periods=len(predictions), freq = 'B')
# line plot
fig = px.line(predictions, x='Date', y=["Low","MA5"], template = 'plotly_dark')
# add a vertical rectange for test-set separation
fig.add_vrect(x0="2022-10-25", x1="2024-03-28", fillcolor="grey", opacity=0.25, line_width=0)
fig.show()

final_best = finalize_model(best)

future_dates = pd.date_range(start = '2024-03-28', end = '2028-03-31', freq = 'B')
future_df = pd.DataFrame()
future_df['Date'] = pd.date_range(start = '2024-03-28', end = '2028-03-31', freq = 'B')
future_df['Series'] = np.arange(2000,(2000+len(future_dates)))
future_df.head()

future_df['MA5'] = df['Low'].rolling(2).mean()

future_df['Date'] = future_df['Date'].dt.strftime('%Y%m%d').astype(int)

predictions_future = predict_model(final_best, data=future_df)
predictions_future.head()

concat_df = pd.concat([df,predictions_future], axis=0)
concat_df_i = pd.date_range(start='2019-04-01', periods=len(concat_df), freq='B')
concat_df.set_index(concat_df_i, inplace=True)
fig = px.line(concat_df, x=concat_df.index, y=["Low", "prediction_label"], template = 'plotly_dark')
fig.show()

df = data.copy()

# create 5 year moving average
df['MA5'] = df['Volume'].rolling(2).mean()
# plot the data and MA
import plotly.express as px
fig = px.line(df, x="Date", y=["Volume", "MA5"], template = 'plotly_dark')
fig.show()

# create a sequence of numbers
df['Series'] = np.arange(1,len(df)+1)
# drop unnecessary columns and re-arrange
df.drop(['Open','Close','High','Low','Adj Close'],axis=1, inplace=True)
df = df[['Series','Date','Volume','MA5']]
# check the head of the dataset
df.head()

df['Date'] = pd.to_datetime(df['Date'], dayfirst = True)
df['Date'] = df['Date'].dt.strftime('%Y%m%d').astype(int)

# split data into train-test set manually
train = df[:900]
test = df[900:]
# check shape
train.shape, test.shape

s = setup(data=train, test_data=test, target='Volume', fold_strategy='timeseries', numeric_features=['Date', 'Series'], fold=3, transform_target=True, fold_shuffle=False, data_split_shuffle=False, session_id=123)

best = create_model('xgboost')

prediction_holdout = predict_model(best);

# generate predictions on the original dataset
predictions = predict_model(best, data=df)
# add a date column in the dataset
predictions['Date'] = pd.date_range(start='2019-04-01', periods=len(predictions), freq = 'B')
# line plot
fig = px.line(predictions, x='Date', y=["Volume","MA5"], template = 'plotly_dark')
# add a vertical rectange for test-set separation
fig.add_vrect(x0="2022-10-25", x1="2024-03-28", fillcolor="grey", opacity=0.25, line_width=0)
fig.show()

final_best = finalize_model(best)

future_dates = pd.date_range(start = '2024-03-28', end = '2028-03-31', freq = 'B')
future_df = pd.DataFrame()
future_df['Date'] = pd.date_range(start = '2024-03-28', end = '2028-03-31', freq = 'B')
future_df['Series'] = np.arange(2000,(2000+len(future_dates)))
future_df.head()

future_df['MA5'] = df['Volume'].rolling(2).mean()

future_df['Date'] = future_df['Date'].dt.strftime('%Y%m%d').astype(int)

predictions_future = predict_model(final_best, data=future_df)
predictions_future.head()

concat_df = pd.concat([df,predictions_future], axis=0)
concat_df_i = pd.date_range(start='2019-04-01', periods=len(concat_df), freq='B')
concat_df.set_index(concat_df_i, inplace=True)
fig = px.line(concat_df, x=concat_df.index, y=["Volume", "prediction_label"], template = 'plotly_dark')
fig.show()

### *Method 4: Vector Autoregression*

from statsmodels.tsa.stattools import adfuller

df = pd.DataFrame(data)
df.set_index('Date', inplace=True)
df.index = pd.date_range(start='2019-04-01', periods=len(df), freq='B')

#plotting the data features together
fig, axes = plt.subplots(nrows=len(df.columns), ncols=1, figsize=(20, 20), sharex=True)

#different colours to differentiate between the features being plotted
colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd',
          '#8c564b']

# Plot each column in a separate subplot
for i, column in enumerate(df.columns):
    axes[i].plot(df.index, df[column], label=column, color=colors[i % len(colors)])
    # axes[i].set_xlabel('Date')
    axes[i].set_ylabel(column)
    axes[i].legend(loc='upper right')
    axes[i].grid(True)

# plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

#Augmented Dickey-Fuller Test on all the columns

def adf_test(series, columns):
  result = adfuller(series)
  print(f'-- ADF Test for {columns} --')
  print('ADF Statistic: %f' % result[0])
  print('p-value: %f' % result[1])
  print('Critical values:')
  for key, value in result[4].items():
    print(f'\t{key}: {value:.2f}')
  if result[1] <= 0.05:
    print(f"Stationary\n")
  else:
    print(f"Non-Stationary\n")

for column in df.columns:
  adf_test(df[column], column)

#Handling non-stationary time series data for VAR modelling

from statsmodels.tsa.holtwinters import Holt

open_price = df['Open']
high_price = df['High']
low_price = df['Low']
close_price = df['Close']
adjclose_price = df['Adj Close']
volume = df['Volume']

open_price_model = Holt(df['Open'])
open_price_results = open_price_model.fit(smoothing_level=0.1)
open_price_trend = open_price_results.fittedvalues
open_price_level = open_price_results.level
open_price_residual = df['Open'] - open_price_trend

high_price_model = Holt(df['High'])
high_price_results = high_price_model.fit(smoothing_level=0.1)
high_price_trend = high_price_results.fittedvalues
high_price_level = high_price_results.level
high_price_residual = df['High'] - high_price_trend

low_price_model = Holt(df['Low'])
low_price_results = low_price_model.fit(smoothing_level=0.1)
low_price_trend = low_price_results.fittedvalues
low_price_level = low_price_results.level
low_price_residual = df['Low'] - low_price_trend

close_price_model = Holt(df['Close'])
close_price_results = close_price_model.fit(smoothing_level=0.1)
close_price_trend = close_price_results.fittedvalues
close_price_level = close_price_results.level
close_price_residual = df['Close'] - close_price_trend

adjclose_price_model = Holt(df['Adj Close'])
adjclose_price_results = adjclose_price_model.fit(smoothing_level=0.1)
adjclose_price_trend = adjclose_price_results.fittedvalues
adjclose_price_level = adjclose_price_results.level
adjclose_price_residual = df['Adj Close'] - adjclose_price_trend

volume_model = Holt(df['Volume'])
volume_results = volume_model.fit(smoothing_level=0.1)
volume_trend = volume_results.fittedvalues
volume_level = volume_results.level
volume_residual = df['Volume'] - volume_trend

#Checking for decomposition in the components using Seasonal Decomposition
from statsmodels.tsa.seasonal import seasonal_decompose

columns = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']
fig, axes = plt.subplots(nrows=len(columns), ncols=1, figsize=(10,15))

#Performing decomposition
for i, column in enumerate(columns):
  decomposition = seasonal_decompose(df[column], model='additive', period=252)
  #plotting original, trend, and residuals
  axes[i].plot(df.index,df[column], label='Original', color='navy')
  axes[i].plot(decomposition.trend, label='Trend', color='orange')
  axes[i].plot(decomposition.resid, label='Residuals', color='lightgreen')

  axes[i].set_title(f'{column.replace("_", " ").title()} Decomposition')
  axes[i].legend(loc='upper right')

plt.tight_layout()
plt.show()

from statsmodels.tsa.api import VAR
from statsmodels.stats.outliers_influence import variance_inflation_factor

data_for_var = pd.concat([df[['High', 'Low', 'Close', 'Volume']],
                          open_price_residual], axis=1, join='inner')

# Calculate VIF to check for multicollinearity
vif_data = pd.DataFrame()
vif_data["Variable"] = data_for_var.columns
vif_data["VIF"] = [variance_inflation_factor(data_for_var.values, i) for i in range(data_for_var.shape[1])]
print(vif_data)



model = VAR(data_for_var)

lag_results = model.select_order(maxlags=10)
print(lag_results.summary())

# Assuming 'data_for_var' contains the relevant time series data
model = VAR(data_for_var)

# Fitting the model with the chosen lag order
model_fitted = model.fit(2, ic='aic', trend='ct')

from statsmodels.stats.stattools import durbin_watson

# Compute Durbin-Watson statistics
dw_stats = durbin_watson(model_fitted.resid)

for col, stat in zip(data_for_var.columns, dw_stats):
    print(f'{col}: {stat}')

print(data_for_var.columns)

# Forecasting
forecast_steps = 10  # Number of steps to forecast ahead
forecast = model_fitted.forecast(model_fitted.endog, steps=forecast_steps)

forecast_df = pd.DataFrame(forecast, index=pd.date_range(start=data_for_var.index[-1] + pd.Timedelta(days=1), periods=forecast_steps, freq='B'), columns=data_for_var.columns)

# Concatenate the predicted and last 50 observations of historical data for 'open_price_residual'
concatenated_data = pd.concat([data_for_var[0].tail(50), forecast_df[0]])

# Plot the concatenated data
plt.figure(figsize=(12, 6))
plt.plot(data_for_var.index[-50:], data_for_var[0].tail(50), label='Historical', color='blue',marker = 'o')
plt.plot(forecast_df.index, forecast_df[0], label='Forecast', color='orange',marker = '*')
plt.title('Historical and Predicted Open Price Residual')
plt.xlabel('Date')
plt.ylabel('Values')
plt.legend()
plt.show()

open_price = df['Open']

# the fitted Holt model
open_price_model = Holt(df['Open'])
open_price_results = open_price_model.fit(smoothing_level=0.1)
open_price_trend = open_price_results.fittedvalues
open_price_level = open_price_results.level
open_price_residual = df['Open'] - open_price_trend


# Forecasting the trend component
forecast_steps = 10  # Number of steps to forecast ahead

# Forecasting future trend values
open_price_trend_forecast = open_price_results.forecast(steps=forecast_steps)

# Creating a date range for the forecast
open_price_trend_forecast_index = pd.date_range(start=open_price_trend.index[-1] + pd.Timedelta(days=1), periods=forecast_steps, freq='B')

# Creating a DataFrame for the forecasted trend
open_price_trend_forecast_df = pd.DataFrame(open_price_trend_forecast, index=open_price_trend_forecast_index, columns=['open_price_trend_forecast'])

# sum the predicted trend and predicted residual values to get the final forecast
forecast_df['open_price_residual_final'] = forecast_df[0] + open_price_trend_forecast_df['open_price_trend_forecast']


# Plotting the final forecast of original open_price
plt.figure(figsize=(12, 6))
plt.plot(open_price[-100:], label='Historical', color='blue')
plt.plot(forecast_df.index, forecast_df['open_price_residual_final'], label='Forecast', color='orange', marker='*')
plt.title('Historical and Predicted Open Price')
plt.xlabel('Date')
plt.ylabel('Values')
plt.legend()
plt.show()

from sklearn.metrics import mean_squared_error

# Get the historical open prices for the last 10 days for comparison
actual_values = open_price[-10:]

# Get the corresponding forecasted open prices
predicted_values = forecast_df['open_price_residual_final']

# Calculate the Mean Squared Error (MSE)
mse = mean_squared_error(actual_values, predicted_values)

print(f'Mean Squared Error: {mse}')

data_for_var = pd.concat([df[['Open', 'Low', 'Close', 'Volume']],
                          high_price_residual], axis=1, join='inner')

# Calculate VIF to check for multicollinearity
vif_data = pd.DataFrame()
vif_data["Variable"] = data_for_var.columns
vif_data["VIF"] = [variance_inflation_factor(data_for_var.values, i) for i in range(data_for_var.shape[1])]
print(vif_data)



model = VAR(data_for_var)

lag_results = model.select_order(maxlags=10)
print(lag_results.summary())

# Assuming 'data_for_var' contains the relevant time series data
model = VAR(data_for_var)

# Fitting the model with the chosen lag order
model_fitted = model.fit(2, ic='aic', trend='ct')

from statsmodels.stats.stattools import durbin_watson

# Compute Durbin-Watson statistics
dw_stats = durbin_watson(model_fitted.resid)

for col, stat in zip(data_for_var.columns, dw_stats):
    print(f'{col}: {stat}')

print(data_for_var.columns)
print(forecast_df.columns)

forecast_df = pd.concat([df[['Open', 'Low', 'Close', 'Volume']],
                          high_price_residual], axis=1, join='inner')

print(data_for_var.columns)
print(forecast_df.columns)

# Forecasting
forecast_steps = 10  # Number of steps to forecast ahead
forecast = model_fitted.forecast(model_fitted.endog, steps=forecast_steps)

forecast_df = pd.DataFrame(forecast, index=pd.date_range(start=data_for_var.index[-1] + pd.Timedelta(days=1), periods=forecast_steps, freq='B'), columns=data_for_var.columns)

# Concatenate the predicted and last 50 observations of historical data for 'high_price_residual'
concatenated_data = pd.concat([data_for_var[0].tail(50), forecast_df[0]])

# Plot the concatenated data
plt.figure(figsize=(12, 6))
plt.plot(data_for_var.index[-50:], data_for_var[0].tail(50), label='Historical', color='blue',marker = 'o')
plt.plot(forecast_df.index, forecast_df[0], label='Forecast', color='orange',marker = '*')
plt.title('Historical and Predicted High Price Residual')
plt.xlabel('Date')
plt.ylabel('Values')
plt.legend()
plt.show()

high_price = df['High']

# the fitted Holt model
high_price_model = Holt(df['High'])
high_price_results = high_price_model.fit(smoothing_level=0.1)
high_price_trend = high_price_results.fittedvalues
high_price_level = high_price_results.level
high_price_residual = df['High'] - high_price_trend


# Forecasting the trend component
forecast_steps = 10  # Number of steps to forecast ahead

# Forecasting future trend values
high_price_trend_forecast = high_price_results.forecast(steps=forecast_steps)

# Creating a date range for the forecast
high_price_trend_forecast_index = pd.date_range(start=high_price_trend.index[-1] + pd.Timedelta(days=1), periods=forecast_steps, freq='B')

# Creating a DataFrame for the forecasted trend
high_price_trend_forecast_df = pd.DataFrame(high_price_trend_forecast, index=high_price_trend_forecast_index, columns=['high_price_trend_forecast'])

# sum the predicted trend and predicted residual values to get the final forecast
forecast_df['high_price_residual_final'] = forecast_df[0] + high_price_trend_forecast_df['high_price_trend_forecast']


# Plotting the final forecast of original high_price
plt.figure(figsize=(12, 6))
plt.plot(high_price[-100:], label='Historical', color='blue')
plt.plot(forecast_df.index, forecast_df['high_price_residual_final'], label='Forecast', color='orange', marker='*')
plt.title('Historical and Predicted High Price')
plt.xlabel('Date')
plt.ylabel('Values')
plt.legend()
plt.show()

from sklearn.metrics import mean_squared_error

# Get the historical high prices for the last 10 days for comparison
actual_values = high_price[-10:]

# Get the corresponding forecasted low prices
predicted_values = forecast_df['high_price_residual_final']

# Calculate the Mean Squared Error (MSE)
mse = mean_squared_error(actual_values, predicted_values)

print(f'Mean Squared Error: {mse}')

data_for_var = pd.concat([df[['Open', 'High', 'Close', 'Volume']],
                          low_price_residual], axis=1, join='inner')

# Calculate VIF to check for multicollinearity
vif_data = pd.DataFrame()
vif_data["Variable"] = data_for_var.columns
vif_data["VIF"] = [variance_inflation_factor(data_for_var.values, i) for i in range(data_for_var.shape[1])]
print(vif_data)

model = VAR(data_for_var)

lag_results = model.select_order(maxlags=10)
print(lag_results.summary())

# Assuming 'data_for_var' contains the relevant time series data
model = VAR(data_for_var)

# Fitting the model with the chosen lag order
model_fitted = model.fit(2, ic='aic', trend='ct')

from statsmodels.stats.stattools import durbin_watson

# Compute Durbin-Watson statistics
dw_stats = durbin_watson(model_fitted.resid)

for col, stat in zip(data_for_var.columns, dw_stats):
    print(f'{col}: {stat}')

print(data_for_var.columns)
print(forecast_df.columns)

forecast_df = pd.concat([df[['Open', 'High', 'Close', 'Volume']],
                          low_price_residual], axis=1, join='inner')

print(data_for_var.columns)
print(forecast_df.columns)

# Forecasting
forecast_steps = 10  # Number of steps to forecast ahead
forecast = model_fitted.forecast(model_fitted.endog, steps=forecast_steps)

forecast_df = pd.DataFrame(forecast, index=pd.date_range(start=data_for_var.index[-1] + pd.Timedelta(days=1), periods=forecast_steps, freq='B'), columns=data_for_var.columns)

# Concatenate the predicted and last 50 observations of historical data for 'high_price_residual'
concatenated_data = pd.concat([data_for_var[0].tail(50), forecast_df[0]])

# Plot the concatenated data
plt.figure(figsize=(12, 6))
plt.plot(data_for_var.index[-50:], data_for_var[0].tail(50), label='Historical', color='blue',marker = 'o')
plt.plot(forecast_df.index, forecast_df[0], label='Forecast', color='orange',marker = '*')
plt.title('Historical and Predicted Low Price Residual')
plt.xlabel('Date')
plt.ylabel('Values')
plt.legend()
plt.show()

low_price = df['Low']

# the fitted Holt model
low_price_model = Holt(df['Low'])
low_price_results = low_price_model.fit(smoothing_level=0.1)
low_price_trend = low_price_results.fittedvalues
low_price_level = low_price_results.level
low_price_residual = df['Low'] - low_price_trend


# Forecasting the trend component
forecast_steps = 10  # Number of steps to forecast ahead

# Forecasting future trend values
low_price_trend_forecast = low_price_results.forecast(steps=forecast_steps)

# Creating a date range for the forecast
low_price_trend_forecast_index = pd.date_range(start=low_price_trend.index[-1] + pd.Timedelta(days=1), periods=forecast_steps, freq='B')

# Creating a DataFrame for the forecasted trend
low_price_trend_forecast_df = pd.DataFrame(low_price_trend_forecast, index=low_price_trend_forecast_index, columns=['low_price_trend_forecast'])

# sum the predicted trend and predicted residual values to get the final forecast
forecast_df['low_price_residual_final'] = forecast_df[0] + low_price_trend_forecast_df['low_price_trend_forecast']


# Plotting the final forecast of original low_price
plt.figure(figsize=(12, 6))
plt.plot(low_price[-100:], label='Historical', color='blue')
plt.plot(forecast_df.index, forecast_df['low_price_residual_final'], label='Forecast', color='orange', marker='*')
plt.title('Historical and Predicted Low Price')
plt.xlabel('Date')
plt.ylabel('Values')
plt.legend()
plt.show()

from sklearn.metrics import mean_squared_error

# Get the historical low prices for the last 10 days for comparison
actual_values = low_price[-10:]

# Get the corresponding forecasted low prices
predicted_values = forecast_df['low_price_residual_final']

# Calculate the Mean Squared Error (MSE)
mse = mean_squared_error(actual_values, predicted_values)

print(f'Mean Squared Error: {mse}')

data_for_var = pd.concat([df[['Open', 'High', 'Low', 'Volume']],
                          close_price_residual], axis=1, join='inner')

# Calculate VIF to check for multicollinearity
vif_data = pd.DataFrame()
vif_data["Variable"] = data_for_var.columns
vif_data["VIF"] = [variance_inflation_factor(data_for_var.values, i) for i in range(data_for_var.shape[1])]
print(vif_data)

model = VAR(data_for_var)

lag_results = model.select_order(maxlags=10)
print(lag_results.summary())

# Assuming 'data_for_var' contains the relevant time series data
model = VAR(data_for_var)

# Fitting the model with the chosen lag order
model_fitted = model.fit(2, ic='aic', trend='ct')

from statsmodels.stats.stattools import durbin_watson

# Compute Durbin-Watson statistics
dw_stats = durbin_watson(model_fitted.resid)

for col, stat in zip(data_for_var.columns, dw_stats):
    print(f'{col}: {stat}')

print(data_for_var.columns)
print(forecast_df.columns)

forecast_df = pd.concat([df[['Open', 'High', 'Low', 'Volume']],
                          close_price_residual], axis=1, join='inner')

print(data_for_var.columns)
print(forecast_df.columns)

# Forecasting
forecast_steps = 10  # Number of steps to forecast ahead
forecast = model_fitted.forecast(model_fitted.endog, steps=forecast_steps)

forecast_df = pd.DataFrame(forecast, index=pd.date_range(start=data_for_var.index[-1] + pd.Timedelta(days=1), periods=forecast_steps, freq='B'), columns=data_for_var.columns)

# Concatenate the predicted and last 50 observations of historical data for 'high_price_residual'
concatenated_data = pd.concat([data_for_var[0].tail(50), forecast_df[0]])

# Plot the concatenated data
plt.figure(figsize=(12, 6))
plt.plot(data_for_var.index[-50:], data_for_var[0].tail(50), label='Historical', color='blue',marker = 'o')
plt.plot(forecast_df.index, forecast_df[0], label='Forecast', color='orange',marker = '*')
plt.title('Historical and Predicted Close Price Residual')
plt.xlabel('Date')
plt.ylabel('Values')
plt.legend()
plt.show()

close_price = df['Close']

# the fitted Holt model
close_price_model = Holt(df['Close'])
close_price_results = close_price_model.fit(smoothing_level=0.1)
close_price_trend = close_price_results.fittedvalues
close_price_level = close_price_results.level
close_price_residual = df['Close'] - close_price_trend


# Forecasting the trend component
forecast_steps = 10  # Number of steps to forecast ahead

# Forecasting future trend values
close_price_trend_forecast = close_price_results.forecast(steps=forecast_steps)

# Creating a date range for the forecast
close_price_trend_forecast_index = pd.date_range(start=close_price_trend.index[-1] + pd.Timedelta(days=1), periods=forecast_steps, freq='B')

# Creating a DataFrame for the forecasted trend
close_price_trend_forecast_df = pd.DataFrame(close_price_trend_forecast, index=close_price_trend_forecast_index, columns=['close_price_trend_forecast'])

# sum the predicted trend and predicted residual values to get the final forecast
forecast_df['close_price_residual_final'] = forecast_df[0] + close_price_trend_forecast_df['close_price_trend_forecast']


# Plotting the final forecast of original close_price
plt.figure(figsize=(12, 6))
plt.plot(close_price[-100:], label='Historical', color='blue')
plt.plot(forecast_df.index, forecast_df['close_price_residual_final'], label='Forecast', color='orange', marker='*')
plt.title('Historical and Predicted Close Price')
plt.xlabel('Date')
plt.ylabel('Values')
plt.legend()
plt.show()

from sklearn.metrics import mean_squared_error

# Get the historical close prices for the last 10 days for comparison
actual_values = close_price[-10:]

# Get the corresponding forecasted close prices
predicted_values = forecast_df['close_price_residual_final']

# Calculate the Mean Squared Error (MSE)
mse = mean_squared_error(actual_values, predicted_values)

print(f'Mean Squared Error: {mse}')

data_for_var = pd.concat([df[['Open', 'High', 'Low', 'Close']],
                          volume_residual], axis=1, join='inner')

# Calculate VIF to check for multicollinearity
vif_data = pd.DataFrame()
vif_data["Variable"] = data_for_var.columns
vif_data["VIF"] = [variance_inflation_factor(data_for_var.values, i) for i in range(data_for_var.shape[1])]
print(vif_data)

model = VAR(data_for_var)

lag_results = model.select_order(maxlags=10)
print(lag_results.summary())

# Assuming 'data_for_var' contains the relevant time series data
model = VAR(data_for_var)

# Fitting the model with the chosen lag order
model_fitted = model.fit(2, ic='aic', trend='ct')

from statsmodels.stats.stattools import durbin_watson

# Compute Durbin-Watson statistics
dw_stats = durbin_watson(model_fitted.resid)

for col, stat in zip(data_for_var.columns, dw_stats):
    print(f'{col}: {stat}')

print(data_for_var.columns)
print(forecast_df.columns)

forecast_df = pd.concat([df[['Open', 'High', 'Low', 'Close']],
                          volume_residual], axis=1, join='inner')

print(data_for_var.columns)
print(forecast_df.columns)

# Forecasting
forecast_steps = 10  # Number of steps to forecast ahead
forecast = model_fitted.forecast(model_fitted.endog, steps=forecast_steps)

forecast_df = pd.DataFrame(forecast, index=pd.date_range(start=data_for_var.index[-1] + pd.Timedelta(days=1), periods=forecast_steps, freq='B'), columns=data_for_var.columns)

# Concatenate the predicted and last 50 observations of historical data for 'high_price_residual'
concatenated_data = pd.concat([data_for_var[0].tail(50), forecast_df[0]])

# Plot the concatenated data
plt.figure(figsize=(12, 6))
plt.plot(data_for_var.index[-50:], data_for_var[0].tail(50), label='Historical', color='blue',marker = 'o')
plt.plot(forecast_df.index, forecast_df[0], label='Forecast', color='orange',marker = '*')
plt.title('Historical and Predicted Volume Residual')
plt.xlabel('Date')
plt.ylabel('Values')
plt.legend()
plt.show()

volume = df['Volume']

# the fitted Holt model
volume_model = Holt(df['Volume'])
volume_results = volume_model.fit(smoothing_level=0.1)
volume_trend = volume_results.fittedvalues
volume_level = volume_results.level
volume_residual = df['Volume'] - volume_trend


# Forecasting the trend component
forecast_steps = 10  # Number of steps to forecast ahead

# Forecasting future trend values
volume_trend_forecast = volume_results.forecast(steps=forecast_steps)

# Creating a date range for the forecast
volume_trend_forecast_index = pd.date_range(start=volume_trend.index[-1] + pd.Timedelta(days=1), periods=forecast_steps, freq='B')

# Creating a DataFrame for the forecasted trend
volume_trend_forecast_df = pd.DataFrame(volume_trend_forecast, index=volume_trend_forecast_index, columns=['volume_trend_forecast'])

# sum the predicted trend and predicted residual values to get the final forecast
forecast_df['volume_residual_final'] = forecast_df[0] + volume_trend_forecast_df['volume_trend_forecast']


# Plotting the final forecast of original volume
plt.figure(figsize=(12, 6))
plt.plot(volume[-100:], label='Historical', color='blue')
plt.plot(forecast_df.index, forecast_df['volume_residual_final'], label='Forecast', color='orange', marker='*')
plt.title('Historical and Predicted Volume Price')
plt.xlabel('Date')
plt.ylabel('Values')
plt.legend()
plt.show()

from sklearn.metrics import mean_squared_error

# Get the historical close prices for the last 10 days for comparison
actual_values = volume[-10:]

# Get the corresponding forecasted close prices
predicted_values = forecast_df['volume_residual_final']

# Calculate the Mean Squared Error (MSE)
mse = mean_squared_error(actual_values, predicted_values)

print(f'Mean Squared Error: {mse}')
